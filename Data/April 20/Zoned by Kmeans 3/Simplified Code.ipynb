{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d3b0ff",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b5e3fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "import glob\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02e18ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cleanUp import cleanUp\n",
    "#from fillDf import fillDf\n",
    "#from fixYearStamp import fixYearStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3d43d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ccffbc",
   "metadata": {},
   "source": [
    "## Cleanup Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c43bdebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanUp takes sensor data in .txt format and transfers it to .csv format whil removing null timestamps and\n",
    "# correcting for user specified time errors in hours.\n",
    "# cutoff: str, formatted according to pandas datetime standards. Will cutoff all data before this time\n",
    "# timeRectifyingParams: dictionary, input dictionary with {condition1:hours to adjust} format in {str:int} datatype\n",
    "# filePaths: iterable with the correct filepaths to look for\n",
    "\n",
    "def autoFix(file,df,start = 0):\n",
    "    indexErrors={}\n",
    "    for idx,i in enumerate(df['Date_Time'][start:]):\n",
    "        try:\n",
    "            pd.Timestamp(i)\n",
    "        except:\n",
    "            print('Error encountered when parsing: ',file)\n",
    "            print('first index',idx,'  ', 'time value \\''+i+'\\'')\n",
    "            # print(len(df['Date_Time']))\n",
    "            indexErrors[idx]=i\n",
    "            df.drop(df[df['Date_Time'] == i].index, inplace = True)\n",
    "            df.reset_index(drop=True)\n",
    "            df = autoFix(file,df,idx)\n",
    "            break\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleanUp(cutoff,timeRectifyingParams,filePaths,columns,badTimes):\n",
    "\n",
    "    fData = {}\n",
    "    mod = {}\n",
    "    cleaningCutOffTime = pd.Timestamp(cutoff)\n",
    "    for idx,file in enumerate(filePaths):\n",
    "        \n",
    "        # Here we are reading in the data from the sensors. if 'all' was put into the columns variable we just\n",
    "        # take everything.\n",
    "        if 'all' in columns:\n",
    "            df = pd.read_csv(\n",
    "                file,\n",
    "                header=1,\n",
    "                parse_dates = [[0,1]]\n",
    "                ).dropna(how='all')\n",
    "        else:\n",
    "            df = pd.read_csv(\n",
    "                file,\n",
    "                header=1,\n",
    "                parse_dates = [[0,1]],\n",
    "                usecols = columns\n",
    "                ).dropna(how='all')\n",
    "        \n",
    "        # This takes annoying spaces out of the column names\n",
    "        df.columns = df.columns.str.replace(' ', '') \n",
    "\n",
    "        # Here we assume the sensor path is in the from ./Data\\\\{sensorname}.txt and shorten it to just be {sensorname}\n",
    "        name = file[7:len(file)-4]\n",
    "\n",
    "        # Here we check to see if the date parse was successful. If not we filter out known buggy timestamps\n",
    "        # if there is a new buggy timestamp that cannot be converted we end the function early and return the pertinent\n",
    "        # info\n",
    "\n",
    "        if type(df['Date_Time'][0]) == type('string'):\n",
    "            for time in badTimes:\n",
    "                df.drop(df[df['Date_Time'] == time].index, inplace = True)\n",
    "            # df.drop(df[df['Date_Time'] == '     0/0/0      0:0:0'].index, inplace = True)\n",
    "            # df.drop(df[df['Date_Time'] == '2165/165/165 165:165:85'].index, inplace = True)\n",
    "            try:\n",
    "                df['Date_Time'] = pd.to_datetime(df['Date_Time'])\n",
    "            except:\n",
    "                df = autoFix(file,df)\n",
    "                df['Date_Time'] = pd.to_datetime(df['Date_Time'])\n",
    "                \n",
    "            # Here we need to set up our time changing parameters\n",
    "            # For this instance we need to roll back all sensors by 1 hour\n",
    "            # except the two BU sensors which needed to be rolled back by\n",
    "            # 8 hours.\n",
    "        try:\n",
    "            offset = timeRectifyingParams[name]\n",
    "            mod[name] = 'yes'\n",
    "            # print(x,'yes')\n",
    "            df['Date_Time'] = df['Date_Time']-pd.Timedelta(hours = offset)\n",
    "        except KeyError:\n",
    "            mod[name] = 'no'\n",
    "            # print(x,'no')\n",
    "\n",
    "        try:\n",
    "            df.drop(df[df['Date_Time'] < cleaningCutOffTime].index, inplace = True)\n",
    "\n",
    "        except TypeError:\n",
    "            print('TypeError: ')\n",
    "            return file,df\n",
    "            # In the instance of a TypeError occuring, we are bascically dealing with\n",
    "            # the 0 timestamps causing an error in the read_csv parser and not \n",
    "            # converting the Date_Time column to timestamp data type.\n",
    "\n",
    "        fData[name] = df.reset_index(drop=True)\n",
    "\n",
    "        # ends by printing out the new start and stop times of the data sets\n",
    "    for label in fData:\n",
    "        try:\n",
    "            print(label,'   ',fData[label]['Date_Time'].iloc[0],'    ',fData[label]['Date_Time'].iloc[-1],'     ','mod:',mod[label])\n",
    "        except:\n",
    "            print(label,' NO DATA PRESENT    NO DATA PRESENT')\n",
    "    return fData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f18f18",
   "metadata": {},
   "source": [
    "## fillDf function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "349a743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillDf(df, freq, start, end, cutoff):\n",
    "    if start:\n",
    "        startTime = pd.Timestamp(start)\n",
    "    else:\n",
    "        startTime = df.values[0][0]\n",
    "\n",
    "    if end:\n",
    "        endTime = pd.Timestamp(end)\n",
    "    else:\n",
    "        endTime = df.values[-1][0] + pd.Timedelta(seconds=freq)\n",
    "\n",
    "    volatility = 0\n",
    "    padding = 0\n",
    "    nochange = 0\n",
    "\n",
    "    threshold = pd.Timedelta(seconds=cutoff)\n",
    "\n",
    "    index = pd.date_range(startTime, endTime, freq=freq)\n",
    "    columns = df.columns\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    overall = []\n",
    "\n",
    "    for idx, i in enumerate(df.values):\n",
    "        oldCount = count\n",
    "\n",
    "        try:\n",
    "            while i[0] >= index[count]:\n",
    "                count += 1\n",
    "        except IndexError:\n",
    "            continue\n",
    "\n",
    "        val = count - oldCount\n",
    "\n",
    "        # if sensor measurements are more frequent than the sampling rate we just skip them\n",
    "        if not val:\n",
    "            continue\n",
    "\n",
    "        if threshold < (index[count] - index[oldCount]):\n",
    "            if not idx:\n",
    "                temp = df.values[0][1:]\n",
    "            else:\n",
    "                temp = df.values[idx - 1][1:]\n",
    "\n",
    "            # if the time gap is over the threshold entries are 0 padded instead of interpolated\n",
    "            for step, ovrwrt in enumerate(range(oldCount, count)):\n",
    "                padding += 1\n",
    "                tempdata = np.concatenate(\n",
    "                    (np.array([index[ovrwrt]]),\n",
    "                     np.floor(np.array(temp * 0))), 0\n",
    "                )\n",
    "                overall.append(tempdata)\n",
    "            val = 0\n",
    "\n",
    "        # might error on first value\n",
    "\n",
    "        if val and val - 1:\n",
    "\n",
    "            # time gaps < threshold will be linearly interpolated\n",
    "\n",
    "            if not idx:\n",
    "                temp = df.values[0][1:]\n",
    "            else:\n",
    "                temp = df.values[idx - 1][1:]\n",
    "            inc = (i[1:] - temp) / val\n",
    "\n",
    "            for step, ovrwrt in enumerate(range(oldCount, count)):\n",
    "                volatility += 1\n",
    "                tempdata = np.concatenate(\n",
    "                    (np.array([index[ovrwrt]]), np.floor(\n",
    "                        np.array(temp + inc * step))),\n",
    "                    0,\n",
    "                )\n",
    "                overall.append(tempdata)\n",
    "\n",
    "        elif val:\n",
    "            nochange += 1\n",
    "            temp = i[1:]\n",
    "            tempdata = np.concatenate(\n",
    "                (np.array([index[oldCount]]), np.floor(np.array(temp))), 0\n",
    "            )\n",
    "            overall.append(tempdata)\n",
    "\n",
    "\n",
    "    total = len(overall)\n",
    "\n",
    "    if total:\n",
    "        accuracy = [\"% of values from interpolation : \" + str(np.round(volatility/total*100, 3)),\n",
    "                    \"% of values from 0-padding : \" +\n",
    "                    str(np.round(padding/total*100, 3)),\n",
    "                    \"% of values not changed : \" + str(np.round(nochange/total*100, 3))]\n",
    "    else:\n",
    "        accuracy = 'NO DATA'\n",
    "\n",
    "    newDF = pd.DataFrame(overall, columns=columns)\n",
    "\n",
    "    return newDF, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8af41",
   "metadata": {},
   "source": [
    "## fixYearStamp function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa51a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixYearStamp(filePath,incorrectString,date,charTimeStart,charTimeEnd,offset):\n",
    "\n",
    "    fin = open(filePath,'rt')\n",
    "    content = fin.readlines()\n",
    "    fin.close()\n",
    "    fout = open(filePath,'wt')\n",
    "    for idx,i in enumerate(content):\n",
    "        if re.search(incorrectString,i):\n",
    "            line = (pd.Timestamp(date+i[charTimeStart:charTimeEnd])-pd.Timedelta(hours=offset)).strftime(' %Y/%m/%d, %H:%M:%S') + i[charTimeEnd:]\n",
    "        else:\n",
    "            line = i\n",
    "        fout.write(line)\n",
    "    fout.close()\n",
    "    return\n",
    "\n",
    "### This function will take in a file, the incorrect date string, the date for it in any format, the pivots for the time char array, and the\n",
    "### hour offset as an integer to correct the function by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf3306",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1916f92",
   "metadata": {},
   "source": [
    "Passing the sensor data through the cleanUp function to get fix timestamps and delete null timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc249fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_csv_files = glob.glob(\"./Data/*.txt\")\n",
    "# insert the desired start time\n",
    "cutOffTime = '4/20/2021 9:30'\n",
    "endTime = '2021-04-20 14:00'\n",
    "# insert the time rectifying offsets. default of for nothing {'':0}\n",
    "sensorConditions = {'S-01':7,'S-02':7,'S-03':7,'S-04':7,'S-05':7,'S-06':7,'S-15':7,'S-19':7}\n",
    "#This indicates which columns to keep. Here we're taking all of the dP info and the timestamps\n",
    "columns = [0,1,6,7,8,9,10,11]\n",
    "# Enable Data Checking\n",
    "DataChecking = False\n",
    "# Here are obversed timestamps that need to removed from the data\n",
    "badTimes = ['     0/0/0      0:0:0','2165/165/165 165:165:85']\n",
    "# Controls wether zones will be created automatically or by k-means clusters\n",
    "ZoneAutomation = False\n",
    "# Sets either the binning or the manual zones\n",
    "numberOfZones = 4\n",
    "numAutoZones = 2\n",
    "# Sensors to exclude from zone\n",
    "outdoorSensors = ['S-15','S-16','S-18','S-19']\n",
    "# 10s of seconds before nebulization to include in the expirement csv files\n",
    "preCursorFactor = 6\n",
    "# which particle to analyze\n",
    "particle = 'Dp>0.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12231861",
   "metadata": {},
   "outputs": [],
   "source": [
    "expTRange = {\n",
    "\n",
    "    'ICU Room 1 Door Partially Open':\n",
    "    [pd.Timestamp('2021-04-20 9:45:15'),\n",
    "    pd.Timestamp('2021-04-20 10:02:40'),\n",
    "    pd.Timestamp('2021-04-20 10:19:40')],\n",
    "    'ICU Room 1 Door Open':\n",
    "    [pd.Timestamp('2021-04-20 10:35:05'),\n",
    "    pd.Timestamp('2021-04-20 10:51:15'),\n",
    "    pd.Timestamp('2021-04-20 11:06:30')],\n",
    "    'ICU Room 1 Negative Pressure':\n",
    "    [pd.Timestamp('2021-04-20 11:25:00'),\n",
    "    pd.Timestamp('2021-04-20 11:37:50'),\n",
    "    pd.Timestamp('2021-04-20 11:47:55')],\n",
    "    'ICU Room 2 Door Partially Open':\n",
    "    [pd.Timestamp('2021-04-20 12:13:35'),\n",
    "    pd.Timestamp('2021-04-20 12:23:30')+pd.Timedelta(140,'S'),\n",
    "    pd.Timestamp('2021-04-20 12:38:30')+pd.Timedelta(190,'S'),\n",
    "    pd.Timestamp('2021-04-20 12:49:45')+pd.Timedelta(190,'S')],\n",
    "    'ICU Room 2 Door Open':\n",
    "    [pd.Timestamp('2021-04-20 13:00:30')+pd.Timedelta(190,'S'),\n",
    "    pd.Timestamp('2021-04-20 13:13:30'),\n",
    "    pd.Timestamp('2021-04-20 13:23:30'),\n",
    "    pd.Timestamp('2021-04-20 13:33:00')],\n",
    "}\n",
    "\n",
    "#enter in the expirement length as seconds/10\n",
    "expTLen = {\n",
    "    'ICU Room 1 Door Partially Open' : 15*6,\n",
    "    'ICU Room 1 Door Open':15*6,\n",
    "    'ICU Room 1 Negative Pressure':10*6,\n",
    "    'ICU Room 2 Door Partially Open':10*6,\n",
    "    'ICU Room 2 Door Open':10*6   \n",
    "}\n",
    "# Manual Zone set up notice how we are missing S-14\n",
    "zoneList = {\n",
    "    'Zone 1' : ['S-01','S-04'],\n",
    "    'Zone 2' : ['S-02','S-03','S-05','S-06'],\n",
    "    'Zone 3' : ['S-07','S-08','S-09','S-10', 'S-12', 'S-13','S-14'], #took out 'S-11' as its data file is missing rn\n",
    "    'Zone 4' : ['S-15','S-18'],\n",
    "    'Zone 5' : ['S-16','S-19']\n",
    "}\n",
    "if not ZoneAutomation:\n",
    "    numberOfZones = len(zoneList)\n",
    "    \n",
    "#all_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e11a7001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-01     2021-04-20 09:30:00      2021-04-20 13:45:59       mod: yes\n",
      "S-02     2021-04-20 09:30:00      2021-04-20 13:46:00       mod: yes\n",
      "S-03     2021-04-20 09:30:00      2021-04-20 13:45:19       mod: yes\n",
      "S-04     2021-04-20 09:30:09      2021-04-20 13:49:09       mod: yes\n",
      "S-05     2021-04-20 09:30:00      2021-04-20 13:43:50       mod: yes\n",
      "S-06     2021-04-20 09:30:00      2021-04-20 13:44:09       mod: yes\n",
      "S-07     2021-04-20 09:30:07      2021-04-20 13:45:27       mod: no\n",
      "S-08     2021-04-20 09:30:09      2021-04-20 13:47:03       mod: no\n",
      "S-09     2021-04-20 09:30:19      2021-04-20 13:44:35       mod: no\n",
      "S-10     2021-04-20 09:30:07      2021-04-20 13:44:23       mod: no\n",
      "S-13     2021-04-20 09:30:08      2021-04-20 13:44:38       mod: no\n",
      "S-14     2021-04-20 09:30:07      2021-04-20 13:43:57       mod: no\n",
      "S-15     2021-04-20 09:30:03      2021-04-20 13:48:44       mod: yes\n",
      "S-16     2021-04-20 09:31:17      2021-04-20 13:49:17       mod: no\n",
      "S-18     2021-04-20 09:30:08      2021-04-20 13:48:39       mod: no\n",
      "S-19     2021-04-20 09:30:02      2021-04-20 13:49:12       mod: yes\n"
     ]
    }
   ],
   "source": [
    "data = cleanUp(cutOffTime,sensorConditions,all_csv_files,columns,badTimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29127b",
   "metadata": {},
   "source": [
    "### Exporting Data\n",
    "Here we can export the organized data frames as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ebff458",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './proccessedData'\n",
    "for x in data:\n",
    "    temp=data[x]\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "360ce51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "fout = open('./dataInfo/interpolation_Effect_Log.txt','wt')\n",
    "interpDF = {}\n",
    "\n",
    "for x in data:\n",
    "    df = data[x]\n",
    "    cutoff = 40\n",
    "    freq = '10S'\n",
    "    try:\n",
    "        interpDF[x],accuracy = fillDf(df,freq,cutOffTime,endTime,cutoff)\n",
    "        #print(x,' ',accuracy)\n",
    "        fout.write(x+' '+ '\\n' + accuracy[0]+ '\\n'+ accuracy[1]+ '\\n'+ accuracy[2] +'\\n\\n')\n",
    "    except IndexError:\n",
    "        #print(x,'NO DATA')\n",
    "        fout.write(x+'NO DATA'+'\\n')\n",
    "fout.close()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f8c19",
   "metadata": {},
   "source": [
    "### Merge the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c5093d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>S-01</th>\n",
       "      <th>S-02</th>\n",
       "      <th>S-03</th>\n",
       "      <th>S-04</th>\n",
       "      <th>S-05</th>\n",
       "      <th>S-06</th>\n",
       "      <th>S-07</th>\n",
       "      <th>S-08</th>\n",
       "      <th>S-09</th>\n",
       "      <th>S-10</th>\n",
       "      <th>S-13</th>\n",
       "      <th>S-14</th>\n",
       "      <th>S-15</th>\n",
       "      <th>S-16</th>\n",
       "      <th>S-18</th>\n",
       "      <th>S-19</th>\n",
       "      <th>Average</th>\n",
       "      <th>Variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-20 09:30:00</td>\n",
       "      <td>69</td>\n",
       "      <td>39</td>\n",
       "      <td>54</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>219</td>\n",
       "      <td>120</td>\n",
       "      <td>1302</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>63</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>27</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>120012.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-20 09:30:10</td>\n",
       "      <td>48</td>\n",
       "      <td>93</td>\n",
       "      <td>54</td>\n",
       "      <td>84</td>\n",
       "      <td>18</td>\n",
       "      <td>81</td>\n",
       "      <td>147</td>\n",
       "      <td>81</td>\n",
       "      <td>1302</td>\n",
       "      <td>66</td>\n",
       "      <td>72</td>\n",
       "      <td>63</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>175.750000</td>\n",
       "      <td>116169.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-20 09:30:20</td>\n",
       "      <td>39</td>\n",
       "      <td>81</td>\n",
       "      <td>48</td>\n",
       "      <td>72</td>\n",
       "      <td>48</td>\n",
       "      <td>99</td>\n",
       "      <td>90</td>\n",
       "      <td>81</td>\n",
       "      <td>750</td>\n",
       "      <td>66</td>\n",
       "      <td>36</td>\n",
       "      <td>45</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "      <td>121.250000</td>\n",
       "      <td>36341.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-20 09:30:30</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>69</td>\n",
       "      <td>39</td>\n",
       "      <td>48</td>\n",
       "      <td>36</td>\n",
       "      <td>126</td>\n",
       "      <td>205</td>\n",
       "      <td>750</td>\n",
       "      <td>69</td>\n",
       "      <td>75</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>57</td>\n",
       "      <td>127.333333</td>\n",
       "      <td>37617.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-20 09:30:40</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "      <td>81</td>\n",
       "      <td>591</td>\n",
       "      <td>427</td>\n",
       "      <td>84</td>\n",
       "      <td>75</td>\n",
       "      <td>54</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>126.083333</td>\n",
       "      <td>30864.576389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>2021-04-20 13:43:10</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.250000</td>\n",
       "      <td>287.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>2021-04-20 13:43:20</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>95.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>2021-04-20 13:43:30</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>331.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>2021-04-20 13:43:40</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>126</td>\n",
       "      <td>30</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>317.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>2021-04-20 13:43:50</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>126</td>\n",
       "      <td>9</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>87.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1524 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date_Time  S-01  S-02  S-03  S-04  S-05  S-06  S-07  S-08  \\\n",
       "0    2021-04-20 09:30:00    69    39    54    36    27    18   219   120   \n",
       "1    2021-04-20 09:30:10    48    93    54    84    18    81   147    81   \n",
       "2    2021-04-20 09:30:20    39    81    48    72    48    99    90    81   \n",
       "3    2021-04-20 09:30:30     9    48    69    39    48    36   126   205   \n",
       "4    2021-04-20 09:30:40    27    39    27    48    42    18    81   591   \n",
       "...                  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "1519 2021-04-20 13:43:10    21     9     9     0    36     0    18     9   \n",
       "1520 2021-04-20 13:43:20     9     9     0     0     9     0     9     0   \n",
       "1521 2021-04-20 13:43:30    18     0     0     0     9     0     0     0   \n",
       "1522 2021-04-20 13:43:40     9     0     9     0    18     0     9     0   \n",
       "1523 2021-04-20 13:43:50     9     0     9     0     9     0     9     9   \n",
       "\n",
       "      S-09  S-10  S-13  S-14  S-15  S-16  S-18  S-19     Average  \\\n",
       "0     1302    21    36    63    33     0    42    27  167.000000   \n",
       "1     1302    66    72    63    75     0    30    27  175.750000   \n",
       "2      750    66    36    45    54     0    57    57  121.250000   \n",
       "3      750    69    75    54    21     0    36    57  127.333333   \n",
       "4      427    84    75    54    39     0    30    27  126.083333   \n",
       "...    ...   ...   ...   ...   ...   ...   ...   ...         ...   \n",
       "1519     0    57     0     0    18     9     0     0   13.250000   \n",
       "1520    28    28    18     4     9     0     0     0    9.500000   \n",
       "1521    66     0     9     0     0     0     0    21    8.500000   \n",
       "1522    66     0     9     0     0    18   126    30   10.000000   \n",
       "1523    33     0    18     0     0     9   126     9    8.000000   \n",
       "\n",
       "           Variance  \n",
       "0     120012.500000  \n",
       "1     116169.687500  \n",
       "2      36341.187500  \n",
       "3      37617.055556  \n",
       "4      30864.576389  \n",
       "...             ...  \n",
       "1519     287.187500  \n",
       "1520      95.750000  \n",
       "1521     331.250000  \n",
       "1522     317.000000  \n",
       "1523      87.500000  \n",
       "\n",
       "[1524 rows x 19 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = []\n",
    "for x in interpDF:\n",
    "    length.append(len(interpDF[x]))\n",
    "index = min(length)\n",
    "lowIDX,lowValue = [[i,value] for i,value in enumerate(length) if value == index][0]\n",
    "\n",
    "columns = list(interpDF.keys())\n",
    "mergedData = pd.DataFrame({'Date_Time':interpDF[columns[lowIDX]]['Date_Time']})\n",
    "for idx,column in enumerate(columns):\n",
    "    mergedData[column] = interpDF[column][particle]\n",
    "Average = np.mean(mergedData[zoneList['Zone 1']+zoneList['Zone 2']+zoneList['Zone 3']],axis=1)\n",
    "Variance = np.var(mergedData[zoneList['Zone 1']+zoneList['Zone 2']+zoneList['Zone 3']],axis=1)\n",
    "mergedData['Average'] = Average\n",
    "mergedData['Variance'] = Variance\n",
    "mergedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9facc71e",
   "metadata": {},
   "source": [
    "## Increase Resolution on Merged Data and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "027f47cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mergedData:\n",
    "    tempFrame = mergedData.values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    hiResMergedDF = pd.DataFrame(tempList, columns = mergedData.keys())\n",
    "    \n",
    "directory = './mergedData/'\n",
    "if not os.path.exists(directory):\n",
    "\n",
    "    os.makedirs(directory)\n",
    "\n",
    "location = os.path.join(directory+'mergedFrame.csv')\n",
    "hiResMergedDF.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd63246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedData = pd.read_csv('./mergedData/mergedFrame.csv',parse_dates=[0])\n",
    "time = mergedData['Date_Time']\n",
    "expIndexes = {}\n",
    "for i in expTRange:\n",
    "    expIndexes[i] = []\n",
    "    for x in expTRange[i]:\n",
    "        for start,n in enumerate(time):\n",
    "           if n >= x:\n",
    "               expIndexes[i].append(start)\n",
    "               break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a3d30",
   "metadata": {},
   "source": [
    "## Determining Zones\n",
    "Here we first create 'averagedFrame's. These are dictionaries that at each 'label' (which corresponds to the name of an expirement) we have a pandas dataframe containing the results of all of the trails in an expirement summed, and then divided by the total number of trails.\n",
    "Anytime you are adjusting the Zones, everything below here must be run. The values of many of these DataFrames are mutated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "def3e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preCursorFactor is defined at the start\n",
    "averagedFrame = {}\n",
    "expirementFrame = {}\n",
    "\n",
    "for label in expIndexes:\n",
    "    runSumFrames = expIndexes[label][0]-expIndexes[label][0]\n",
    "    for idx,time in enumerate(expIndexes[label]):\n",
    "        start = expIndexes[label][idx] - preCursorFactor\n",
    "        end = expIndexes[label][idx] + expTLen[label]\n",
    "        expirementFrame[label+' Exp '+str(idx+1)] = mergedData.iloc[ start : end , 1: ].reset_index(drop = True)\n",
    "        runSumFrames += expirementFrame[label+' Exp '+str(idx+1)]\n",
    "        \n",
    "    averagedFrame[label] = runSumFrames/(idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20568fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numAutoZones is defined at the start\n",
    "AutoZoneAssignments = {}\n",
    "for frame in averagedFrame:\n",
    "    # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "    avgFrm = averagedFrame[frame]\n",
    "    # outdoorSensors must have its spelling exactly match\n",
    "    columns = list(set(avgFrm.keys()[:-2])- set(outdoorSensors))\n",
    "    columns.sort()\n",
    "\n",
    "    X = {}\n",
    "    for column in columns:\n",
    "        value,index = max([(value,index) for index,value in enumerate(avgFrm[column])]) \n",
    "        X[column] = np.array([np.log(value+.01),index])\n",
    "    X = [X[i] for i in X]\n",
    "    kmeans = KMeans(n_clusters=numAutoZones,random_state=0).fit(X)\n",
    "    idx = np.argsort(kmeans.cluster_centers_.sum(axis=1))\n",
    "    lut = np.zeros_like(idx)\n",
    "    lut[idx] = np.arange(numAutoZones)\n",
    "    #lut = lut[::-1]\n",
    "    orderedZones = [[]]*numAutoZones\n",
    "    for index, zone in enumerate(lut):\n",
    "        orderedZones[index] = [index if zone == kmeans.labels_[i] else 0 for i in range(len(kmeans.labels_))]\n",
    "    AutoZoneAssignments[frame] = np.sum(orderedZones,axis=0)\n",
    "z = numAutoZones\n",
    "ZDfAuto = pd.DataFrame(AutoZoneAssignments)\n",
    "ZDfAuto = ZDfAuto.append(pd.DataFrame([[z]*len(expIndexes)]*len(outdoorSensors),columns = AutoZoneAssignments.keys()),ignore_index=True)\n",
    "AutoZoneAssignments = ZDfAuto\n",
    "if len(outdoorSensors):\n",
    "    numAutoZones += 1\n",
    "\n",
    "if not ZoneAutomation:\n",
    "    ZoneAssignments = {}\n",
    "    for frame in averagedFrame:\n",
    "        # at this point averagedFrame should just be the averaged sum of the expirementFrame trails. Last two columns are overall average and varaince so they should be ignored.\n",
    "        avgFrm = averagedFrame[frame]\n",
    "        # outdoorSensors must have its spelling exactly match\n",
    "        columns = list(set(avgFrm.keys()[:-2]))\n",
    "        columns.sort()\n",
    "        ZoneAssignments[frame] = [0]*len(columns)\n",
    "        for value,zone in enumerate(zoneList):\n",
    "            for sensor in zoneList[zone]:\n",
    "                ZoneAssignments[frame][columns.index(sensor)] = value\n",
    "    ZDf = pd.DataFrame(ZoneAssignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05b4c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'ZoneAssignments.csv')\n",
    "ZDf.to_csv(location,index=False)\n",
    "\n",
    "directory = './dataInfo'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "location = os.path.join(directory,'AutoZoneAssignments.csv')\n",
    "ZDfAuto.to_csv(location,index=False)\n",
    "\n",
    "expirementFrameAuto = copy.deepcopy(expirementFrame)\n",
    "averagedFrameAuto = copy.deepcopy(averagedFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfce3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "stretchedDF = {}\n",
    "for i in averagedFrame:\n",
    "    tempFrame = averagedFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDF[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns)\n",
    "\n",
    "stretchExpDf = {}\n",
    "for i in expirementFrame:\n",
    "    tempFrame = expirementFrame[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDf[i] = pd.DataFrame(tempList, columns = expirementFrame[list(expirementFrame.keys())[0]].columns) \n",
    "stretchedDFAuto = {}\n",
    "for i in averagedFrameAuto:\n",
    "    tempFrame = averagedFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchedDFAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns) \n",
    "\n",
    "stretchExpDfAuto = {}\n",
    "for i in expirementFrameAuto:\n",
    "    tempFrame = expirementFrameAuto[i].values\n",
    "    tempList = []\n",
    "    for idx,x in enumerate(tempFrame):\n",
    "        try:\n",
    "            increment = (tempFrame[idx+1] - x)/10\n",
    "            for count in range(10):\n",
    "                tempList.append(x+increment*count)\n",
    "        except IndexError:\n",
    "            tempList.append(x)\n",
    "            continue\n",
    "    stretchExpDfAuto[i] = pd.DataFrame(tempList, columns = expirementFrameAuto[list(expirementFrameAuto.keys())[0]].columns)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db6bed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './stretchedAvgData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDF:\n",
    "    temp=stretchedDF[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementData'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDf:\n",
    "    temp=stretchExpDf[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedAvgDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchedDFAuto:\n",
    "    temp=stretchedDFAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)\n",
    "directory = './stretchedExpirementDataAuto'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "for x in stretchExpDfAuto:\n",
    "    temp=stretchExpDfAuto[x]\n",
    "    location = os.path.join(directory,x+'.csv')\n",
    "    temp.to_csv(location,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11066d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
